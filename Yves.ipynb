{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import src.dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_Y(target):\n",
    "    '''\n",
    "    Create one-hot labels for Y\n",
    "    '''\n",
    "    res = torch.zeros(target.size(0), 2)\n",
    "    res[range(target.size(0)), target] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_Class(target):\n",
    "    res = torch.zeros(target.size(0), 10*target.size(1))\n",
    "    res[range(target.size(0)), target[:, 0]] = 1\n",
    "    res[range(target.size(0)), target[:, 1] + 10] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "train_X, train_Y, train_Class, test_X, test_Y, test_Class = prologue.generate_pair_sets(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_Y = onehot_Y(train_Y).long()\n",
    "#train_Class = onehot_Class(train_Class).long()\n",
    "#test_Y = onehot_Y(test_Y).long()\n",
    "#test_Class = onehot_Class(test_Class).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]],\n",
       "\n",
       "         [[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]]],\n",
       "\n",
       "\n",
       "        [[[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]],\n",
       "\n",
       "         [[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]]],\n",
       "\n",
       "\n",
       "        [[[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]],\n",
       "\n",
       "         [[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]],\n",
       "\n",
       "         [[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]]],\n",
       "\n",
       "\n",
       "        [[[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]],\n",
       "\n",
       "         [[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]]],\n",
       "\n",
       "\n",
       "        [[[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653,  0.1419,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.0722,  ..., -0.4653, -0.4653, -0.4653]],\n",
       "\n",
       "         [[-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          ...,\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653],\n",
       "          [-0.4653, -0.4653, -0.4653,  ..., -0.4653, -0.4653, -0.4653]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, std = train_X.mean(), train_X.std()\n",
    "train_X.sub_(mu).div_(std)\n",
    "test_X.sub_(mu).div_(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Naive convnet\n",
    "For the first model, we create a naive convnet, not taking into account the structrue of the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, nb_epochs, minibatch_size, train_X, train_Y, verbose=False):\n",
    "    for e in range(nb_epochs):\n",
    "        for b in range(0, train_X.size(0), minibatch_size):\n",
    "            out = model(train_X.narrow(0, b, minibatch_size))\n",
    "            loss = criterion(out, train_Y.narrow(0, b, minibatch_size))\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if(verbose): print(loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, minibatch_size):\n",
    "    nb_data_errors = 0\n",
    "    for b in range(0, data_input.size(0), minibatch_size):\n",
    "        out = model(data_input.narrow(0, b, minibatch_size))\n",
    "        _, pred = torch.max(out.data, 1)\n",
    "        for k in range(minibatch_size):\n",
    "            if data_target[b+k] != pred[k]:\n",
    "                nb_data_errors += 1\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6794, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6529, grad_fn=<NllLossBackward>)\n",
      "tensor(0.6240, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5779, grad_fn=<NllLossBackward>)\n",
      "tensor(0.5367, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4974, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4523, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4379, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3877, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4385, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3271, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4138, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2939, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2768, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2392, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2876, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2588, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1905, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2463, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2017, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1575, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1148, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1374, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1222, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0807, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1204, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0481, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0207, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0173, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0149, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0129, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0112, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0088, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0079, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0072, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0066, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0061, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0056, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0052, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0049, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0046, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0043, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0041, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0038, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0036, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0034, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0033, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0031, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0028, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0027, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0026, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0024, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0022, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0020, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0019, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0018, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0017, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0016, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0015, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0012, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0010, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0008, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model1 = convNet()\n",
    "model1 = train_model(model1, nn.CrossEntropyLoss(), optim.SGD(model1.parameters(), lr=1e-1), 100, 100, train_X, train_Y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model1, test_X, test_Y, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that this model does not manage to learn the mapping very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Study of a good functioning digit detection network for 28x28 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = \\\n",
    "    prologue.load_data(one_hot_labels = False, normalize = True, flatten = False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target = Variable(train_input), Variable(train_target)\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = train_model(model, nn.CrossEntropyLoss(), optim.SGD(model.parameters(), lr=1e-1), 50, 100, train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model, test_input, test_target, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Adapting the network to 14x14 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fist, create the training and testing dataset\n",
    "train_target_14px = torch.cat((train_Class[:,0], train_Class[:,1]))\n",
    "train_input_14px = torch.cat((train_X[:,0,:,:].resize_(1000,1,14,14), train_X[:,1,:,:].resize_(1000,1,14,14)))\n",
    "\n",
    "test_target_14px = torch.cat((test_Class[:,0], test_Class[:,1]))\n",
    "test_input_14px = torch.cat((test_X[:,0,:,:].resize_(1000,1,14,14), test_X[:,1,:,:].resize_(1000,1,14,14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_14px(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_14px, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3) \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2)) #image size 12x12-> image size 6x6\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2)) #image size 4x4 -> image size 2x2\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3059, grad_fn=<NllLossBackward>)\n",
      "tensor(2.3013, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2983, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2950, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2923, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2885, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2851, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2827, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2801, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2767, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2731, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2693, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2662, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2625, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2586, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2548, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2511, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2466, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2419, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2367, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2308, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2248, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2186, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2112, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2022, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1947, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1862, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1758, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1681, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1546, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1599, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1311, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1214, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1066, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1067, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0774, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0793, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0341, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0624, grad_fn=<NllLossBackward>)\n",
      "tensor(2.0056, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9993, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9947, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9621, grad_fn=<NllLossBackward>)\n",
      "tensor(1.9188, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8936, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8669, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8506, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8321, grad_fn=<NllLossBackward>)\n",
      "tensor(1.8203, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7686, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7580, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7762, grad_fn=<NllLossBackward>)\n",
      "tensor(1.7136, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6861, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6771, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6702, grad_fn=<NllLossBackward>)\n",
      "tensor(1.6246, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5828, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5691, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5721, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4724, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4655, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3745, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3245, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3657, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3884, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2999, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2928, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4171, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2493, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1447, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4964, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1866, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0838, grad_fn=<NllLossBackward>)\n",
      "tensor(1.3162, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2645, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1058, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0795, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2326, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0157, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0716, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0730, grad_fn=<NllLossBackward>)\n",
      "tensor(1.0187, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8556, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9784, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1785, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9659, grad_fn=<NllLossBackward>)\n",
      "tensor(1.1220, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4329, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8671, grad_fn=<NllLossBackward>)\n",
      "tensor(1.2382, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9025, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8165, grad_fn=<NllLossBackward>)\n",
      "tensor(0.9755, grad_fn=<NllLossBackward>)\n",
      "tensor(0.7815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8443, grad_fn=<NllLossBackward>)\n",
      "tensor(0.8770, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model_14px = Net_14px()\n",
    "model_14px = train_model(model_14px, nn.CrossEntropyLoss(), optim.SGD(model_14px.parameters(), lr=1e-1), 100, 100, \\\n",
    "                         train_input_14px, train_target_14px, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1781"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nb_errors(model_14px, test_input_14px, test_target_14px, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
